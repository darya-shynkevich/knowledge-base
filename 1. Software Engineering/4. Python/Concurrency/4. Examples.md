# Blocking I/O

So you and 2 of your friends walked into McDonald’s. you ordered a first meal. ***your friend must wait until your order is completed***.

This is a blocking model. The whole process is slow and easy to understand.

![Pasted image 20231015195238](../../../_Attachments/Pasted%20image%2020231015195238.png)

In CS terms.
- “You” and 2 friends are `function calls`.
- the “Queue” is the `process` itself.
- “Waiter” is the `main thread` (in `user space`) of the `process`.
- “Kitchen” is the `kernel space`.
- The “Cook” is the `kernel thread` which doing the task.

# Multi-threading

**==*Short: one McDonald, everyone holds an empty plate standing in front of the cook*==**

> Note: the below model is not for Python.

![Pasted image 20231015195450](../../../_Attachments/Pasted%20image%2020231015195450.png)

- Kitchen is `kernel space`. The Cook is the `CPU thread`.
- You and 2 of your friends are `3 threads`, executed together.
- You are all competing with each other. Standing in front of the cook holding an empty plate and chasing the cook to make it faster.
- The cook trying to finish everything ASAP. But he forgets things, he needs to refresh his mind to recall what each of you orders (`context switch`)
- So ***he finished a bit of thing at a time but not in order***. like first filling up your drink, next finishing your friend’s burger, then your 2nd friend’s drink, then back to your burger, and so on.

As you can see: ***threading is competing with each other. instead of coordinating***.

# Multi-threading + GIL

![Pasted image 20231015195823](../../../_Attachments/Pasted%20image%2020231015195823.png)

- It is still a “mess”: ***everyone competition each other’s order***
- It is even worse: the “waiter” limit only one order (`thread`) could be made (`executed`) at a time.
- So overall, ***this is slower than the above model.***

# [12. Coroutines](12.%20Coroutines.md)

***==Short: one Mcdonald, one waiter coordinating orders==***

We want better control of concurrency, instead of competing with each other.

So here we have Coroutine. which is `asyncio` in Python. but, ***it is not multi-threaded. just one thread.***

![Pasted image 20231015200014](../../../_Attachments/Pasted%20image%2020231015200014.png)

==***Compared to threading. we have better control. Exiting a function is easier than killing a thread, no context switch, or resource racing***.==

Wait. since asyncio can also archive concurrency in Python, with just one thread, then for I/O-bound tasks what’s the point of using multi-threading in Python (due to GIL, it is also kind of one thread)? => *==**not all the I/O operations support the async interface**==*: *when you connect to a server using library X. and the X doesn’t have to await functions to call. in this case, the best option in Python still using multi-threading.*

# Multi-processing

***==Short: go to multiple Mcdonald’s==***

So this time, you all decided to eat faster. so you 3 walked into different McDonald’s separately and meet again after eating.

![Pasted image 20231015214915](../../../_Attachments/Pasted%20image%2020231015214915.png)

- You, F1, and F2 are 3 `processes`
- The waiter is the `main thread`
- “Cook” is the `CPU thread`

For the “`CPU bound`” task, ==***using `multi-processing` is always faster than “I/O bound***==”? ***==Yes, but only in Python==***

Threads can also be served by multiple CPUs. The point to use multi-processing is not for speed. Mainly for:
- Clean. Clear separation (memory address, executing stack, register states, etc) without worrying about resource racing, makes code much easier to manage.
- System command calls.
- Easier to manage. Once started a process, got the PID and can kill it or wait for output. But it’s harder to kill or monitor a thread execution.

*So, there is a chance using multi-threading could archive similar speed even for “`CPU-bound`” tasks? yes, maybe in c++, java, c# but not in Python (due to GIL).*